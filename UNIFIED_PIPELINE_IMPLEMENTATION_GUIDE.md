# Cricket Data Processing - Unified Pipeline Implementation Guide

## üìã Table of Contents
- [Overview](#overview)
- [Current State Analysis](#current-state-analysis)
- [Proposed Solution Architecture](#proposed-solution-architecture)
- [Implementation Plan](#implementation-plan)
- [Detailed Component Specifications](#detailed-component-specifications)
- [Step-by-Step Implementation Instructions](#step-by-step-implementation-instructions)
- [Testing Strategy](#testing-strategy)
- [Deployment Guide](#deployment-guide)
- [Troubleshooting](#troubleshooting)

---

## üìñ Overview

### What We're Building
A unified, automated pipeline that processes cricket data from Cricsheet JSON files into a fully populated PostgreSQL database with all analytical columns and statistics.

### Why We Need This
Currently, processing new cricket match data requires running 7+ separate scripts manually, with CSV exports/imports in between. This is:
- Time-consuming and error-prone
- Leaves the database in inconsistent states
- Requires manual intervention for player data
- Not optimized for our 1.6M+ delivery records

### What Success Looks Like
```bash
# Single command to process new matches
python unified_pipeline.py --process-new-matches /path/to/json/files/

# Single command to refresh existing data
python unified_pipeline.py --refresh-all-data

# Result: Fully processed database with all columns populated
```

---

## üîç Current State Analysis

### Existing Scripts and Their Roles

| Script | Purpose | Input | Output | Issues |
|--------|---------|-------|--------|--------|
| `loadMatches.py` | Load basic match/delivery data | JSON files | Matches + Deliveries (basic) | Missing new columns |
| `statsProcessor.py` | Generate batting/bowling stats | Deliveries | BattingStats + BowlingStats | Fails if players missing |
| `venue_standardization.py` | Clean venue names | Matches | Updated venue names | Standalone process |
| `update_player_info_enhanced.py` | Find/fix missing players | Database | CSV + Updates | Manual CSV editing |
| `delivery_column_updater.py` | Populate delivery columns | Players + Deliveries | Enhanced delivery columns | Newly created |
| `update_crease_combo_granular.py` | Enhance crease combinations | Deliveries | Granular crease combos | One-time migration |

### Current Processing Flow Problems
1. **Fragmented**: Each script runs independently
2. **Manual**: Requires human intervention for player data
3. **Incomplete**: New matches missing enhanced columns
4. **Inefficient**: Multiple database passes for same data
5. **Error-prone**: No coordination between scripts

---

## üèóÔ∏è Proposed Solution Architecture

### High-Level Architecture
```
üìÅ JSON Files ‚Üí üéØ UNIFIED PIPELINE ‚Üí ‚úÖ Complete Database
```

### Component Overview
```
unified_pipeline.py (Orchestrator)
‚îú‚îÄ‚îÄ player_discovery.py (Auto-discover missing players)
‚îú‚îÄ‚îÄ enhanced_loadMatches.py (Load with all columns)
‚îú‚îÄ‚îÄ delivery_column_updater.py (Refresh delivery columns)
‚îú‚îÄ‚îÄ enhanced_statsProcessor.py (Generate statistics)
‚îî‚îÄ‚îÄ data_validator.py (Validate completeness)
```

### Processing Phases
1. **Discovery Phase**: Find missing players, prepare data
2. **Loading Phase**: Load matches with all columns populated
3. **Enhancement Phase**: Update any missing columns
4. **Statistics Phase**: Generate all statistics
5. **Validation Phase**: Verify data completeness

---

## üìã Implementation Plan

### Phase 1: Core Infrastructure (Week 1)
- [ ] Create `player_discovery.py`
- [ ] Enhance existing `loadMatches.py` ‚Üí `enhanced_loadMatches.py`
- [ ] Create `data_validator.py`
- [ ] Test components individually

### Phase 2: Pipeline Orchestration (Week 2)
- [ ] Create `unified_pipeline.py`
- [ ] Integrate all components
- [ ] Add configuration management
- [ ] Add comprehensive logging

### Phase 3: Enhancement & Optimization (Week 3)
- [ ] Optimize bulk operations
- [ ] Add error recovery
- [ ] Create user documentation
- [ ] Performance testing

### Phase 4: Testing & Deployment (Week 4)
- [ ] Test with subset of data
- [ ] Full database refresh test
- [ ] Documentation finalization
- [ ] Production deployment

---

## üîß Detailed Component Specifications

### 1. `player_discovery.py`
**Purpose**: Automatically discover and create missing player entries

**Key Functions**:
```python
def scan_json_files_for_players(json_directory: str) -> Set[str]
def find_missing_players(all_players: Set[str]) -> Dict[str, PlayerInfo]
def create_placeholder_players(missing_players: Dict[str, PlayerInfo]) -> int
def generate_missing_players_report(missing_players: Dict[str, PlayerInfo]) -> str
```

**Logic**:
1. Scan all JSON files to extract player names
2. Compare with existing players table
3. Create placeholder entries with auto-discovered info (nationality from team, etc.)
4. Flag players needing manual review

### 2. `enhanced_loadMatches.py`
**Purpose**: Enhanced version of existing loadMatches.py with full column population

**Key Enhancements**:
- Populate delivery enhancement columns during initial load
- Integrate with player discovery
- Bulk optimization for all operations
- Handle both new matches and backfill scenarios

**Key Functions**:
```python
def load_match_with_enhanced_columns(json_file: str, session: Session) -> bool
def populate_delivery_columns(deliveries: List[Delivery], session: Session) -> None
def bulk_insert_with_enhancements(matches: List[Match], deliveries: List[Delivery]) -> None
```

### 3. `data_validator.py`
**Purpose**: Validate data completeness and generate reports

**Key Functions**:
```python
def validate_delivery_columns() -> ValidationReport
def validate_player_completeness() -> ValidationReport
def validate_statistics_coverage() -> ValidationReport
def generate_data_quality_report() -> str
```

### 4. `unified_pipeline.py`
**Purpose**: Orchestrate all processing components

**Key Functions**:
```python
def process_new_matches(json_directory: str) -> PipelineResults
def refresh_existing_data() -> PipelineResults
def update_player_data_and_refresh(player_updates: List[str]) -> PipelineResults
def run_full_pipeline(mode: str, **kwargs) -> PipelineResults
```

---

## üìù Step-by-Step Implementation Instructions

### Step 1: Create Player Discovery Service

#### 1.1 Create the File
```bash
touch /Users/adityabalaji/cdt/cricket-data-thing/player_discovery.py
```

#### 1.2 Core Structure
```python
#!/usr/bin/env python3
"""
Player Discovery Service

Automatically discovers missing players from JSON files and creates
placeholder entries in the players table.
"""

import json
import os
from pathlib import Path
from typing import Dict, Set, List, Optional, NamedTuple
from dataclasses import dataclass
from sqlalchemy.orm import Session
from database import get_database_connection
from models import Player

@dataclass
class PlayerInfo:
    name: str
    appears_as: List[str]  # ['batter', 'bowler', 'both']
    team_countries: Set[str]  # Teams they've played for
    match_count: int
    ball_count: int
    likely_nationality: Optional[str] = None

class PlayerDiscoveryService:
    def __init__(self):
        # Implementation details here
        pass
    
    def scan_json_files_for_players(self, json_directory: str) -> Dict[str, PlayerInfo]:
        """Scan all JSON files and extract player information"""
        # Implementation details here
        pass
    
    def find_missing_players(self, discovered_players: Dict[str, PlayerInfo]) -> Dict[str, PlayerInfo]:
        """Compare discovered players with database and find missing ones"""
        # Implementation details here
        pass
    
    def create_placeholder_players(self, missing_players: Dict[str, PlayerInfo]) -> int:
        """Create placeholder entries for missing players"""
        # Implementation details here
        pass
```

#### 1.3 Implementation Details
Focus on these key methods:

**Method 1: `scan_json_files_for_players`**
- Loop through all JSON files in directory
- Extract batter, non_striker, bowler from each delivery
- Track team associations for nationality inference
- Count appearances and contexts

**Method 2: `find_missing_players`**
- Query existing players table
- Compare with discovered players
- Return difference with metadata

**Method 3: `create_placeholder_players`**
- Create Player objects with discovered info
- Set nationality based on team patterns
- Mark records as auto-discovered
- Bulk insert for efficiency

### Step 2: Enhance LoadMatches Script

#### 2.1 Copy and Enhance Existing Script
```bash
cp /Users/adityabalaji/cdt/cricket-data-thing/dataloader/loadMatches.py /Users/adityabalaji/cdt/cricket-data-thing/enhanced_loadMatches.py
```

#### 2.2 Key Enhancements to Add

**Enhanced Delivery Creation**:
```python
def create_enhanced_delivery(ball_data: dict, match_context: dict, session: Session) -> Delivery:
    """Create delivery with all enhancement columns populated"""
    delivery = Delivery(
        # ... existing fields ...
    )
    
    # Populate enhancement columns during creation
    delivery.striker_batter_type = get_player_batter_type(ball_data['batter'], session)
    delivery.non_striker_batter_type = get_player_batter_type(ball_data['non_striker'], session)
    delivery.bowler_type = get_player_bowler_type(ball_data['bowler'], session)
    
    # Calculate derived columns
    delivery.crease_combo = calculate_crease_combo(delivery.striker_batter_type, delivery.non_striker_batter_type)
    delivery.ball_direction = calculate_ball_direction(delivery.striker_batter_type, delivery.bowler_type)
    
    return delivery
```

**Player Integration**:
```python
def ensure_players_exist(json_file: str, session: Session) -> None:
    """Ensure all players from match exist in players table"""
    discovery_service = PlayerDiscoveryService()
    missing_players = discovery_service.scan_single_file(json_file)
    if missing_players:
        discovery_service.create_placeholder_players(missing_players)
```

### Step 3: Create Data Validator

#### 3.1 Create Validation Framework
```python
#!/usr/bin/env python3
"""
Data Validator

Validates data completeness and generates quality reports.
"""

from dataclasses import dataclass
from typing import Dict, List, Optional
from sqlalchemy import text
from database import get_database_connection

@dataclass
class ValidationResult:
    component: str
    status: str  # 'PASS', 'WARN', 'FAIL'
    message: str
    details: Optional[Dict] = None

class DataValidator:
    def __init__(self):
        self.engine, SessionLocal = get_database_connection()
        self.session = SessionLocal()
    
    def validate_delivery_columns(self) -> List[ValidationResult]:
        """Validate delivery enhancement columns"""
        results = []
        
        # Check striker_batter_type completeness
        query = text("""
            SELECT 
                COUNT(*) as total,
                COUNT(striker_batter_type) as populated,
                COUNT(CASE WHEN striker_batter_type = 'unknown' THEN 1 END) as unknown
            FROM deliveries
        """)
        # ... implementation
        
        return results
    
    def validate_player_completeness(self) -> List[ValidationResult]:
        """Validate player data completeness"""
        # ... implementation
        pass
    
    def generate_report(self) -> str:
        """Generate comprehensive data quality report"""
        # ... implementation
        pass
```

### Step 4: Create Unified Pipeline Orchestrator

#### 4.1 Core Pipeline Structure
```python
#!/usr/bin/env python3
"""
Unified Cricket Data Pipeline

Orchestrates all data processing components for complete automation.
"""

import argparse
from dataclasses import dataclass
from typing import Dict, List, Optional
from datetime import datetime
from player_discovery import PlayerDiscoveryService
from enhanced_loadMatches import EnhancedMatchLoader
from delivery_column_updater import DeliveryColumnUpdater
from data_validator import DataValidator

@dataclass
class PipelineResults:
    success: bool
    duration: float
    matches_processed: int
    deliveries_processed: int
    players_added: int
    errors: List[str]
    warnings: List[str]
    validation_report: str

class UnifiedPipeline:
    def __init__(self, config: Optional[Dict] = None):
        self.config = config or self._load_default_config()
        self.setup_logging()
    
    def process_new_matches(self, json_directory: str) -> PipelineResults:
        """Process new matches with full pipeline"""
        # Phase 1: Discovery
        # Phase 2: Loading  
        # Phase 3: Enhancement
        # Phase 4: Statistics
        # Phase 5: Validation
        pass
    
    def refresh_existing_data(self) -> PipelineResults:
        """Refresh all delivery columns and statistics"""
        pass
    
    def update_players_and_refresh(self, player_names: List[str]) -> PipelineResults:
        """Update specific players and refresh affected deliveries"""
        pass
```

#### 4.2 Configuration Management
Create `pipeline_config.json`:
```json
{
    "batch_sizes": {
        "match_loading": 100,
        "delivery_updates": 25000,
        "statistics": 50
    },
    "timeouts": {
        "discovery_phase": 300,
        "loading_phase": 1800,
        "enhancement_phase": 600
    },
    "validation": {
        "required_columns": ["striker_batter_type", "bowler_type", "crease_combo"],
        "min_completeness_percent": 95
    },
    "logging": {
        "level": "INFO",
        "file": "pipeline.log"
    }
}
```

### Step 5: Integration and Testing

#### 5.1 Unit Testing Each Component
Create test files for each component:
```bash
touch test_player_discovery.py
touch test_enhanced_loadMatches.py
touch test_data_validator.py
touch test_unified_pipeline.py
```

#### 5.2 Integration Testing
```python
# test_integration.py
def test_full_pipeline_with_sample_data():
    """Test complete pipeline with small dataset"""
    # Create test JSON files
    # Run pipeline
    # Validate results
    pass

def test_pipeline_error_recovery():
    """Test pipeline behavior with errors"""
    # Introduce errors
    # Verify graceful handling
    pass
```

#### 5.3 Performance Testing
```python
# test_performance.py
def test_bulk_operations_performance():
    """Test performance with large datasets"""
    # Measure processing time
    # Verify memory usage
    # Check database locks
    pass
```

---

## üß™ Testing Strategy

### Unit Testing Approach
Each component should have comprehensive unit tests covering:
- Happy path scenarios
- Error conditions
- Edge cases (empty data, malformed JSON, etc.)
- Performance with large datasets

### Integration Testing Approach
- Test complete pipeline with small dataset (10-20 matches)
- Test pipeline with realistic dataset (1000+ matches)
- Test error recovery and rollback scenarios
- Test incremental vs. full processing modes

### Performance Testing Approach
- Benchmark against current manual process
- Test with full 1.6M delivery dataset
- Monitor memory usage and database performance
- Verify bulk operations efficiency

---

## üöÄ Deployment Guide

### Development Environment Setup
1. Create feature branch: `git checkout -b unified-pipeline`
2. Install any new dependencies: `pip install -r requirements.txt`
3. Run unit tests: `python -m pytest tests/`
4. Test with sample data

### Staging Environment Testing
1. Deploy to staging database
2. Run full pipeline test with subset of production data
3. Validate results against current manual process
4. Performance benchmarking

### Production Deployment
1. Create database backup: `pg_dump cricket_db > backup_before_pipeline.sql`
2. Deploy new code
3. Run initial full refresh: `python unified_pipeline.py --refresh-all-data`
4. Validate results and performance
5. Update documentation and processes

---

## üîß Configuration Management

### Environment Variables
```bash
# Database configuration
export CRICKET_DB_URL="postgresql://user:pass@localhost/cricket_db"
export PIPELINE_BATCH_SIZE=25000
export PIPELINE_LOG_LEVEL=INFO

# Processing configuration  
export JSON_DATA_PATH="/path/to/cricsheet/json/"
export PIPELINE_TEMP_DIR="/tmp/cricket_pipeline/"
```

### Configuration Files
- `pipeline_config.json`: Main pipeline configuration
- `player_discovery_config.json`: Player discovery settings
- `bulk_operations_config.json`: Batch size and performance settings

---

## üêõ Troubleshooting

### Common Issues and Solutions

#### Issue: "Player not found" errors during loading
**Symptoms**: Pipeline fails during match loading
**Cause**: Player discovery didn't create all needed players
**Solution**: 
1. Check player discovery logs
2. Manually run player discovery on failed files
3. Verify players table has required entries

#### Issue: Slow bulk operations
**Symptoms**: Pipeline takes too long to complete
**Cause**: Batch sizes too large or database contention
**Solution**:
1. Reduce batch sizes in configuration
2. Check database indexes on frequently joined columns
3. Monitor database locks during processing

#### Issue: Inconsistent delivery column data
**Symptoms**: Some deliveries have null enhancement columns
**Cause**: Players added after initial delivery loading
**Solution**:
1. Run delivery column updater for specific players
2. Check player data completeness
3. Re-run enhancement phase if needed

### Debugging Tips
1. **Enable verbose logging**: Set `PIPELINE_LOG_LEVEL=DEBUG`
2. **Use smaller datasets**: Test with 10-20 matches first
3. **Check database state**: Validate each phase completion
4. **Monitor performance**: Use database query logs to find bottlenecks

### Error Recovery
The pipeline includes rollback capabilities:
```bash
# Rollback failed pipeline run
python unified_pipeline.py --rollback --transaction-id abc123

# Resume from specific phase
python unified_pipeline.py --resume --from-phase enhancement
```

---

## üìä Monitoring and Maintenance

### Performance Metrics to Track
- Processing time per phase
- Deliveries processed per minute
- Memory usage during bulk operations
- Database lock duration

### Regular Maintenance Tasks
- Monitor player data completeness (weekly)
- Validate delivery column accuracy (monthly)
- Performance optimization review (quarterly)
- Update player discovery rules as needed

### Alerts and Notifications
Set up monitoring for:
- Pipeline failures
- Performance degradation
- Data quality issues
- Disk space usage during processing

---

## üìö Additional Resources

### Documentation References
- PostgreSQL bulk operations: https://www.postgresql.org/docs/current/sql-copy.html
- SQLAlchemy bulk operations: https://docs.sqlalchemy.org/en/14/orm/persistence_techniques.html
- Python logging best practices: https://docs.python.org/3/howto/logging.html

### Code Examples
Check the `examples/` directory for:
- Sample configuration files
- Unit test examples
- Performance benchmarking scripts
- Error handling patterns

---

## ‚úÖ Implementation Checklist

### Phase 1: Core Components
- [ ] `player_discovery.py` - Auto-discover missing players
- [ ] `enhanced_loadMatches.py` - Enhanced data loading
- [ ] `data_validator.py` - Data quality validation
- [ ] Unit tests for each component

### Phase 2: Pipeline Integration
- [ ] `unified_pipeline.py` - Main orchestrator
- [ ] Configuration management system
- [ ] Logging and monitoring setup
- [ ] Integration tests

### Phase 3: Optimization & Polish
- [ ] Performance optimization
- [ ] Error recovery mechanisms
- [ ] Documentation completion
- [ ] User guides and examples

### Phase 4: Deployment & Testing
- [ ] Staging environment testing
- [ ] Performance benchmarking
- [ ] Production deployment
- [ ] Monitoring setup

---

**Next Steps**: Start with Phase 1, Component 1 - `player_discovery.py`. Would you like me to begin implementing this first component?
